# RadiantLogic Common Services Configuration
# ------------------------------------------

# This configuration defines a suite of services for managing and monitoring a RadiantLogic deployment.
# It includes tools for GitOps (Argo CD), observability (Prometheus, Grafana), log management (Elasticsearch, Kibana),
# database administration (PostgreSQL, pgAdmin), load balancing (HAProxy), and other supporting services.

# The following sections configure individual components.
# Enable or disable them as needed by setting `enabled` to `true` or `false`.

# Note: Adjust individual component settings to match your specific environment and requirements.

nodeSelector: {}

# Ingress Configuration for Load Balancing
# ----------------------------------------

# This section defines an Ingress resource to manage external access to services within your Kubernetes cluster.

# Key Features:
# - `className`: Specifies the Ingress controller to use ("alb" in this case, likely for AWS Application Load Balancer).
# - `hosts`: Lists the hostnames that the Ingress should respond to.
# - `paths`: Defines the URL paths that are routed to the corresponding services.
# - `tls`: Configures TLS certificates for secure HTTPS communication (optional).

# Note: Ensure the specified Ingress controller is installed and that the TLS configuration (if used) is valid.

ingress:
  enabled: false
  className: alb
  annotations: {}
  hosts:
    - host: chart-example.local
      paths:
        - "/"
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

zookeeper:
  enabled: false
  fullnameOverride: zookeeper
  nodeSelector: {}

# Argo CD Configuration
# ---------------------
#
# This section enables and configures Argo CD, a GitOps continuous delivery tool for Kubernetes.
#
# Key Features:
# - Automated application deployment and synchronization from Git repositories
# - Declarative configuration using Kubernetes manifests
# - Visual UI for managing applications and observing their state
# - Optional integration with Dex for authentication and authorization
# - Customizable server settings (root path, security)
# - Node selector configuration for different Argo CD components
#
# Note: Review and adjust settings for production environments.

argo-cd:
  enabled: true
  fullnameOverride: argocd
  configs:
    params:
      server.rootpath: "/argocd"
      server.insecure: true
  dex:
    enabled: false
  applicationSet:
    enabled: false
  notifications:
    enabled: false
  controller:
    nodeSelector: {}
  redis:
    nodeSelector: {}
  server:
    nodeSelector: {}
    service:
      type: NodePort
  repoServer:
    nodeSelector: {}
  crds:
    keep: false

# Prometheus Monitoring Stack Configuration
# ----------------------------------------
#
# This section enables deployment and configuration of Prometheus components:
#
# - Prometheus Server: Core time-series database for metrics collection and querying.
# - Pushgateway (Optional):  Allows ephemeral and batch jobs to expose metrics.
# - Node Exporter, Kube State Metrics, Alertmanager (Optional): Additional components for node-level metrics, Kubernetes state metrics, and alerting respectively.
#
# Key Features:
# - Customizable flags for Prometheus Server
# - Persistent storage options for Prometheus Server
# - Ability to enable/disable individual components
# - Configurable nodeSelectors for pod placement
#
# Note: Ensure correct external URL for Prometheus Server and adjust storage as needed.

prometheus:
  enabled: true
  server:
    extraFlags:
      - web.enable-lifecycle
      - web.route-prefix=/
      - web.external-url=http://prometheus-server/prometheus/
      - web.enable-admin-api
    fullnameOverride: prometheus-server
    configmapReload:
      enabled: false
    nodeSelector: {}
    # Persistence enabled by default and size to 50Gi
    persistentVolume:
      size: 8Gi
    statefulSet:
      enabled: true
  nodeExporter:
    enabled: false
  kubeStateMetrics:
    enabled: false
  alertmanager:
    enabled: false

  pushgateway:
    fullnameOverride: prometheus-pushgateway
    nodeSelector: {}
    extraArgs:
      web.enable-admin-api: true

# Grafana Configuration
# ---------------------
#
# This section deploys and configures a Grafana instance, including:
#
# - Persistent storage for dashboards and data
# - Data sources for Prometheus, Elasticsearch, Alertmanager, and OpenSearch
# - Dashboard provisioning from files (with customizable update intervals)
# - Configurable settings in `grafana.ini` (e.g., root URL, authentication)
# - Ability to override the Grafana name (`fullnameOverride`)
# - Configurable node selection and persistent storage options
# - Email (SMTP) setup for notifications (if enabled)
#
# Note: Ensure correct data source URLs and credentials. Place dashboard JSON files in the specified directories.


grafana:
  fullnameOverride: grafana
  enabled: true
  nodeSelector: {}
  # Persistence enabled by default
  persistence:
    enabled: true
    size: 8Gi
  grafana.ini:
    server:
      root_url: "%(protocol)s://%(domain)s/eoc-backend/grafana"
      serve_from_sub_path: true
    auth.anonymous:
      enabled: true
      org_role: Viewer
    analytics:
      check_for_updates: false
    panels:
      disable_sanitize_html: true
    log:
      mode: console
    log.console:
      format: text
      level: info
    security:
      allow_embedding: true
    smtp:
      enabled: true
      host: smtp-server:25
      from_address: saas@radiantlogic.com
    live:
      allowed_origins: "*"
  # Setup Data Source (prometheus and elastic)
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          url: http://prometheus-server
          access: proxy
          isDefault: true
        - name: Elasticsearch
          type: elasticsearch
          database: vds_server_access.log*
          url: http://elasticsearch-master:9200
          password: ""
          user: ""
          access: proxy
          isDefault: false
          jsonData:
            esVersion: '7.17.3'
            logLevelField: fields.level
            logMessageField: message
            maxConcurrentShardRequests: 5
            timeField: '@timestamp'
          readonly: true
        - name: OpenSearch
          type: opensearch
          access: proxy
          isDefault: false
          url: http://opensearch-cluster-master:9200
          jsonData:
            esVersion: 70
            timeField: "@timestamp"
            interval: Daily
          readonly: true

  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'fid'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        updateIntervalSeconds: 10
        allowUiUpdates: true
        editable: true
        options:
          path: /var/lib/grafana/dashboards/fid
      - name: 'zookeeper'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        updateIntervalSeconds: 10
        allowUiUpdates: true
        editable: true
        options:
          path: /var/lib/grafana/dashboards/zookeeper
      - name: 'elasticsearch'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        updateIntervalSeconds: 10
        allowUiUpdates: true
        editable: true
        options:
          path: /var/lib/grafana/dashboards/elasticsearch
      - name: 'service-status'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        updateIntervalSeconds: 10
        allowUiUpdates: true
        editable: true
        options:
          path: /var/lib/grafana/dashboards/service-status
      - name: 'ia-service-status'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        updateIntervalSeconds: 10
        allowUiUpdates: true
        editable: true
        options:
          path: /var/lib/grafana/dashboards/ia-service-status
      - name: 'ida'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        updateIntervalSeconds: 10
        allowUiUpdates: true
        editable: true
        options:
          path: /var/lib/grafana/dashboards/ida
      - name: 'ia-config-webapp'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        updateIntervalSeconds: 10
        allowUiUpdates: true
        editable: true
        options:
          path: /var/lib/grafana/dashboards/ia-config-webapp
      - name: ia-controller-beam
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        updateIntervalSeconds: 10
        allowUiUpdates: true
        editable: true
        options:
          path: /var/lib/grafana/dashboards/ia-controller-beam
      - name: 'ia-controller-ecto'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        updateIntervalSeconds: 10
        allowUiUpdates: true
        editable: true
        options:
          path: /var/lib/grafana/dashboards/ia-controller-ecto
      - name: 'ia-controller-oban'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        updateIntervalSeconds: 10
        allowUiUpdates: true
        editable: true
        options:
          path: /var/lib/grafana/dashboards/ia-controller-oban
      - name: 'ia-controller-phoenix'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        updateIntervalSeconds: 10
        allowUiUpdates: true
        editable: true
        options:
          path: /var/lib/grafana/dashboards/ia-controller-phoenix
      - name: 'ia-data-ingestion'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        updateIntervalSeconds: 10
        allowUiUpdates: true
        editable: true
        options:
          path: /var/lib/grafana/dashboards/ia-data-ingestion
      - name: 'ia-extractor-webapp'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        updateIntervalSeconds: 10
        allowUiUpdates: true
        editable: true
        options:
          path: /var/lib/grafana/dashboards/ia-extractor-webapp
      - name: 'ia-governance-portal'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        updateIntervalSeconds: 10
        allowUiUpdates: true
        editable: true
        options:
          path: /var/lib/grafana/dashboards/ia-governance-portal
  dashboardsConfigMaps:
    fid: "fid-dashboard"
    zookeeper: "zookeeper-dashboard"
    elasticsearch: "audit-logs-elastic-dashboard"
    service-status: "service-status-dashboard"
    ia-service-status: "ia-service-status-dashboard"
    ida: "ia-dashboard"
    ia-config-webapp: "ia-config-webapp-dashboard"
    ia-controller-beam: "ia-controller-beam-dashboard"
    ia-controller-ecto: "ia-controller-ecto-dasboard"
    ia-controller-oban: "ia-controller-oban-dashboard"
    ia-controller-phoenix: "ia-controller-phoenix-dashboard"
    ia-data-ingestion: "ia-data-ingestion-dashboard"
    ia-extractor-webapp: "ia-extractor-webapp-dashboard"
    ia-governance-portal: "ia-governance-portal-dashboard"

# Elasticsearch & Kibana Configuration
# -----------------------------------
#
# This section enables deployment of an Elasticsearch cluster and Kibana for log management and visualization.
#
# Key Features:
# - Single-node Elasticsearch cluster (customizable replica count)
# - Persistent storage for Elasticsearch data
# - Kibana instance with customizable base path and telemetry settings
# - Configurable node selectors for pod placement
#
# Important Considerations:
# - Adjust storage size (`volumeClaimTemplate.resources.requests.storage`) to match your needs.
# - Modify `clusterHealthCheckParams` based on your cluster's expected health status and timeout.

elasticsearch:
  enabled: true
  replicas: 1
  nodeSelector: {}
  # Persistence enabled by default and size to 100Gi
  volumeClaimTemplate:
    resources:
      requests:
        storage: 30Gi
  clusterHealthCheckParams: "wait_for_status=yellow&timeout=60s"
  # enable authentication
  # esMajorVersion: 6
  # minimumMasterNodes: 1
  # esConfig:
  #   elasticsearch.yml: |
  #     xpack.security.enabled: true
  #     discovery.type: single-node
  # extraEnvs:
  #   - name: ELASTIC_PASSWORD
  #     value: changeme

kibana:
  enabled: true
  fullnameOverride: kibana
  nodeSelector: {}
  # extraEnvs:
  #   - name: ELASTICSEARCH_USERNAME
  #     value: elastic
  #   - name: ELASTICSEARCH_PASSWORD
  #     value: changeme
  healthCheckPath: "/eoc-backend/kibana"
  kibanaConfig:
    kibana.yml: |
      server.basePath: "/eoc-backend/kibana"
      server.publicBaseUrl: http://kibana.{{ .Release.Namespace }}.svc.cluster.local:5601/eoc-backend/kibana
      telemetry.optIn: false
      security.showInsecureClusterWarning: false
      server.rewriteBasePath: true
      server.compression.enabled: true
      server.requestId.allowFromAnyIp: true      

# HAProxy Configuration
# ---------------------
#
# This section configures HAProxy as an API gateway for internal services.
# It provides:
#   - Load balancing and routing for enabled services
#   - Basic security headers and connection timeouts
#   - Stats and health check endpoints
#   - Customization through the 'route' section (toggle services on/off)
#   - Fine-grained backend configuration using templating
#
# Note: Ensure correct service names and ports in the backend definitions.

haproxy:
  enabled: true
  route:
    argocd: true
    grafana: true
    prometheus: false
    pushgateway: false
    elasticsearch: false
    kibana: true
    pgadmin4: false
    slamd: false
    shellinabox: false
    eocui: false
    eocapi: false
    sdcapi: false
    opensearch: false
    opensearchdashboards: false

  fullnameOverride: haproxy
  nodeSelector: {}
  service:
    type: NodePort
  config: |
    defaults
      timeout connect 10s
      timeout client 30s
      timeout server 30s
      log global
      mode http
      option httplog
      maxconn 3000
    frontend http-in
      bind *:80

      stats enable
      stats refresh 30s
      stats show-node
      stats uri /stats
      monitor-uri /healthz

      # Remove unnecessary headers
      http-response del-header Server
      http-response del-header X-Powered-By
      http-response del-header X-AspNetMvc-Version
      http-response del-header X-AspNet-Version
      http-response del-header X-Drupal-Cache
      http-response del-header X-Drupal-Dynamic-Cache
      http-response del-header X-Generator
      http-response del-header X-Runtime
      http-response del-header X-Rack-Cache

      # Add security headers
      http-response set-header Strict-Transport-Security "max-age=16000000; includeSubDomains; preload;"
      http-response set-header X-Frame-Options "SAMEORIGIN"
      http-response set-header X-Content-Type-Options "nosniff"
      http-response set-header Referrer-Policy no-referrer-when-downgrade
      http-response set-header X-XSS-Protection 1;mode=block
      http-response set-header Permissions-Policy interest-cohort=()


      # routing
      {{- if ((.Values.route).argocd | default false) }}
      use_backend argocd_backend if { path /argocd } or { path_beg /argocd/ }
      {{- end }}
      {{- if ((.Values.route).grafana | default false) }}
      use_backend grafana_backend if { path /grafana } or { path_beg /grafana/ }
      {{- end }}
      {{- if ((.Values.route).prometheus | default false) }}
      use_backend prometheus_backend if { path /prometheus } or { path_beg /prometheus/ }
      {{- end }}
      {{- if ((.Values.route).pushgateway | default false) }}
      use_backend pushgateway_backend if { path /pushgateway } or { path_beg /pushgateway/ }
      {{- end }}
      {{- if ((.Values.route).kibana | default false) }}
      use_backend kibana_backend if { path /kibana } or { path_beg /kibana/ }
      {{- end }}
      {{- if ((.Values.route).elasticsearch | default false) }}
      use_backend elasticsearch_backend if { path /elasticsearch } or { path_beg /elasticsearch/ }
      {{- end }}
      {{- if ((.Values.route).pgadmin4 | default false) }}
      use_backend pgadmin4_backend if { path /pgadmin4 } or { path_beg /pgadmin4/ }
      {{- end }}
      {{- if ((.Values.route).slamd | default false) }}
      use_backend slamd_backend if { path /slamd } or { path_beg /slamd/ }
      {{- end }}
      {{- if ((.Values.route).shellinabox | default false) }}
      use_backend shellinabox_backend if { path /shellinabox } or { path_beg /shellinabox/ }
      {{- end }}
      {{- if ((.Values.route).eocui | default false) }}
      use_backend eocui_backend if { path /eoc } or { path_beg /eoc/ }
      {{- end }}
      {{- if ((.Values.route).eocapi | default false) }}
      use_backend eocapi_backend if { path /eoc-backend } or { path_beg /eoc-backend/ }
      {{- end }}
      {{- if ((.Values.route).sdccui | default false) }}
      use_backend sdcapi_backend if { path /sdc } or { path_beg /sdc/ }
      {{- end }}
      {{- if ((.Values.route).opensearchdashboards | default false) }}
      use_backend opensearchdashboards_backend if { path /opensearch-dashboards } or { path_beg /opensearch-dashboards/ }
      {{- end }}
      {{- if ((.Values.route).opensearch | default false) }}
      use_backend opensearch_backend if { path /opensearch } or { path_beg /opensearch/ }
      {{- end }}

    # backends
    {{- if ((.Values.route).argocd | default false) }}
    backend argocd_backend
      server argocd argocd-server:80 check
    {{- end }}
    {{- if ((.Values.route).grafana | default false) }}
    backend grafana_backend
      http-request set-path %[path,regsub(^/grafana/?,/)]
      server grafana grafana:80 check
    {{- end }}
    {{- if ((.Values.route).prometheus | default false) }}
    backend prometheus_backend
      http-request set-path %[path,regsub(^/prometheus/?,/)]
      server prometheus prometheus-server:80 check
    {{- end }}
    {{- if ((.Values.route).pushgateway | default false) }}
    backend pushgateway_backend
      http-request set-path %[path,regsub(^/pushgateway/?,/)]
      server pushgateway prometheus-pushgateway:9091 check
    {{- end }}
    {{- if ((.Values.route).kibana | default false) }}
    backend kibana_backend
      http-request set-path %[path,regsub(^/kibana/?,/)]
      server kibana kibana:5601 check
    {{- end }}
    {{- if ((.Values.route).elasticsearch | default false) }}
    backend elasticsearch_backend
      http-request set-path %[path,regsub(^/elasticsearch/?,/)]
      server elasticsearch elasticsearch-master:9200 check
    {{- end }}
    {{- if ((.Values.route).pgadmin4 | default false) }}
    backend pgadmin4_backend
      server pgadmin4 pgadmin4:80 check
    {{- end }}
    {{- if ((.Values.route).slamd | default false) }}
    backend slamd_backend
      server slamd slamd:80 check
    {{- end }}
    {{- if ((.Values.route).shellinabox | default false) }}
    backend shellinabox_backend
      http-request set-path %[path,regsub(^/shellinabox/?,/)]
      server shellinabox shellinabox:8080 check
    {{- end }}
    {{- if ((.Values.route).eocui | default false) }}
    backend eocui_backend
      server eocui eoc-ui-service:80 check
    {{- end }}
    {{- if ((.Values.route).eocapi | default false) }}
    backend eocapi_backend
      server eocapi eoc-backend-service:80 check
    {{- end }}
    {{- if ((.Values.route).sdccui | default false) }}
    backend sdcapi_backend
      server sdcapi sdc-agent:80 check
    {{- end }}
    {{- if ((.Values.route).opensearchdashboards | default false) }}
    backend opensearchdashboards_backend
      #http-request set-path %[path,regsub(^/opensearch-dashboards/?,/)]
      server opensearchdashboards opensearch-dashboards:5601 check
    {{- end }}
    {{- if ((.Values.route).opensearch | default false) }}
    backend opensearch_backend
      http-request set-path %[path,regsub(^/opensearch/?,/)]
      server opensearch opensearch-cluster-master:9200 check
    {{- end }}

# PostgreSQL Database Configuration
# ---------------------------------
#
# This section enables deployment and configuration of a PostgreSQL database cluster.
#
# Key Features:
# - Single-node PostgreSQL deployment (can be expanded in more advanced scenarios)
# - Persistent storage for database data
# - Customizable initialization scripts via ConfigMap
# - Predefined databases for EOC and SDC applications
# - Option to configure backups to S3 (commented out by default)
#
# Important Considerations:
# - Adjust storage size (`persistence.size`) to match your needs.
# - Securely manage database credentials (e.g., using Kubernetes Secrets).
# - If enabling backups, ensure proper S3 configuration and credentials.

postgresql:
  enabled: true
  fullnameOverride: postgresql
  primary:
    nodeSelector: {}
    # Persistence enabled by default and size to 50Gi
    persistence:
      size: 10Gi
    initdb:
      scriptsConfigMap: "postgres-init-script"
  databases:
    eoc:
      databaseName: eocdb
      user: eocadmin
      password: TSXojYsPF4AeZgTq
      schema: eoc
    sdc:
      databaseName: agentsdb
      user: agentsadmin
      password: iJukleKLG9fNihIQ
      schema: agents
  # backup:
    # enabled: false
    # s3bucketName: ""
    # folderName: "postgresql"
    # latestBackupName: "postgresql-latest.sql.gz"

# pgAdmin4 Configuration
# ----------------------
#
# This section enables deployment of pgAdmin4, a web-based administration tool for PostgreSQL databases.
#
# Key Features:
# - Single-pod deployment of pgAdmin4
# - Configurable context path for web access
# - Option to enable persistent storage for configuration data
# - Customizable node selection for pod placement
#
# Note: Persistent storage is disabled by default. Enable it if you need to preserve pgAdmin4 configuration data across pod restarts.

pgadmin4:
  enabled: true
  fullnameOverride: pgadmin4
  nodeSelector: {}
  persistentVolume:
    enabled: false
  env:
    contextPath: "/pgadmin4"

# SLAMD Configuration
# --------------------
#
# This section enables deployment of SLAMD, a tool for load testing and benchmarking LDAP directories.
#
# Key Features:
# - Deploy SLAMD server and optionally client instances
# - Customize image versions and pull policies
# - Configure pod and container security settings
# - Define resource requests and limits for SLAMD components
# - Optionally enable autoscaling based on CPU utilization
# - Configure node selectors, tolerations, and affinity for pod placement
#
# Note: Review and adjust settings according to your testing requirements.

slamd:
  enabled: true
  replicaCount: 1
  image:
    repository: pgodey/slamd
    pullPolicy: IfNotPresent
    tag: "latest"
  imagePullSecrets: []
  podAnnotations: {}
  podSecurityContext: {}
    # fsGroup: 2000
  securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000
  service:
    type: ClusterIP
    port: 80
  resources: {}
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80
  nodeSelector: {}
  tolerations: []
  affinity: {}

  client:
    replicaCount: 0
    image:
      repository: pgodey/slamd-client
      pullPolicy: IfNotPresent
      tag: "latest"
    podAnnotations: {}
    podSecurityContext: {}
    # fsGroup: 2000
    securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000
    resources: {}
    autoscaling:
      enabled: false
      minReplicas: 1
      maxReplicas: 100
      targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80
    nodeSelector: {}
    tolerations: []
    affinity: {}

# Shellinabox Configuration
# ------------------------
#
# This section enables deployment of Shellinabox, a web-based SSH terminal emulator.
#
# Key Features:
# - Single-pod deployment of Shellinabox
# - Customizable Docker image repository and tag
# - Configurable service type and port for access
# - Optional settings for pod and container security
# - Resource management options for CPU and memory
# - Node selection, tolerations, and affinity controls for pod scheduling
#
# Note: Review and adjust security settings based on your environment's requirements.

shellinabox:
  enabled: true
  replicaCount: 1
  image:
    repository: sspreitzer/shellinabox
    pullPolicy: IfNotPresent
    tag: "ubuntu"
  imagePullSecrets: []
  podAnnotations: {}
  podSecurityContext: {}
    # fsGroup: 2000
  securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000
  service:
    type: ClusterIP
    port: 8080
  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# SMTP Relay Configuration
# ------------------------
#
# This section enables deployment of an SMTP relay container to send emails from within the Kubernetes cluster.
#
# Key Features:
# - Single-pod deployment of SMTP relay
# - Configurable SMTP relay host, port, and authentication credentials
# - Customizable Docker image repository and tag
# - Configurable service type and port for access
# - Optional settings for pod and container security
# - Resource management options for CPU and memory
# - Node selection, tolerations, and affinity controls for pod scheduling
#
# Note: Ensure correct SMTP relay credentials and adjust security settings based on your environment's requirements.

smtp:
  enabled: true
  replicaCount: 1
  image:
    repository: bytemark/smtp
    tag: "latest"
    pullPolicy: IfNotPresent
  imagePullSecrets: []
  podAnnotations: {}
  podSecurityContext: {}
    # fsGroup: 2000
  securityContext: {}
  service:
    type: ClusterIP
    port: 8080
  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}
  relay:
    enabled: true
    host: "smtp.sendgrid.net"
    port: "587"
    username: ""
    password: ""

# OpenSearch Configuration
# ------------------------
# This section controls the deployment of an OpenSearch cluster and its accompanying dashboard.
#
# Key Settings:
#   - `enabled`: Set to `true` to deploy OpenSearch (and optionally OpenSearch Dashboards).
#   - `singleNode`: If `true`, a single-node cluster is created for development/testing.
#   - `replicas`: Number of data replicas for each primary shard (for high availability and scalability).
#   - `persistence.size`: Size of the persistent volume for storing OpenSearch data.

opensearch:
  enabled: false
  fullnameOverride: "opensearch"
  singleNode: true
  replicas: 1
  clusterName: "opensearch-cluster"
  nodeGroup: "master"
  masterService: "opensearch-cluster-master"
  extraEnvs:
    - name: "DISABLE_SECURITY_PLUGIN"
      value: "true"
    - name: "DISABLE_INSTALL_DEMO_CONFIG"
      value: "true"
  rbac:
    create: false
    serviceAccountAnnotations: {}
    serviceAccountName: ""
  nodeSelector: {}
  persistence:
    size: 30Gi
  podSecurityContext:
    fsGroup: 1000
    runAsUser: 1000
  service:
    type: ClusterIP
    nodePort: ""
    annotations: {}
    httpPortName: http
    transportPortName: transport

opensearch-dashboards:
  enabled: false
  opensearchHosts: "http://opensearch-cluster-master:9200"
  replicaCount: 1
  fullnameOverride: "opensearch-dashboards"
  extraEnvs:
    - name: DISABLE_SECURITY_DASHBOARDS_PLUGIN
      value: "true"
  service:
    type: ClusterIP
    port: 5601
    loadBalancerIP: ""
    nodePort: ""
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    # 0.0.0.0/0
    httpPortName: http
  nodeSelector: {}
    # tenantname: duploservices-nike-svc
  plugins:
    enabled: false
    installList: []
  config:
    opensearch_dashboards.yml: |
      server:
        basePath: "/opensearch-dashboards"
        rewriteBasePath: true

# Fluent Bit Configuration
# ------------------------
# This section controls the deployment and configuration of Fluent Bit, a lightweight and high-performance log processor and forwarder.

# Enable/Disable Fluent Bit: Set `enabled` to `true` to deploy Fluent Bit.
# Node Selection (Optional): Use `nodeSelector` to schedule Fluent Bit pods onto specific nodes.
# Log Sources (Required): Define a list of log sources that Fluent Bit should collect.
#    - For each source, specify:
#        - `name`: A descriptive name for the log source.
#        - `enable`: Whether to collect logs from this source (`true` or `false`).
#        - `path`: The path to the log files (use wildcards like `*`).
#        - `refresh_interval`: How often Fluent Bit checks for new log data (in seconds).
# Output Destination (Required): Configure where Fluent Bit should send the collected logs.
#    - `outputSearchHost`: Hostname or IP of your Elasticsearch (or OpenSearch) cluster.
#    - `outputSearchType`:  Choose either "es" for Elasticsearch or "opensearch".
# Fluent Bit Configuration (Optional): Fine-tune Fluent Bit's behavior.
#    - `fullnameOverride`: Customize the name of Fluent Bit resources (e.g., DaemonSet).
#    - `flush`: How often Fluent Bit sends logs to the output destination (in seconds).
#    - `logLevel`:  Logging verbosity ("info", "debug", etc.).
#    - `metricsPort`: Port for exposing Fluent Bit metrics.
# Service (Optional): Expose Fluent Bit's metrics or other ports via a Kubernetes Service.
# Monitoring (Optional):
#    - `prometheusRule`: Enable to create a PrometheusRule for Fluent Bit metrics.
#    - `serviceMonitor`: Enable to create a ServiceMonitor to scrape Fluent Bit metrics with Prometheus Operator.
# Existing ConfigMap (Optional): If you have a pre-existing ConfigMap with custom Fluent Bit configuration, specify its name here.

# fluent-bit:
#   enabled: false
#   nodeSelector: {}
#     # tenantname:
#   logs:
#     - name: eoc-backend
#       enable: true
#       path: /var/log/containers/eoc-backend-*.log
#       refresh_interval: 10
#     - name: eoc-orchestrator
#       enable: true
#       path: /var/log/containers/eoc-orchestrator-*.log
#       refresh_interval: 10
#     - name: sdc
#       enable: true
#       path: /var/log/containers/sdc-*.log
#       refresh_interval: 10
#     - name: client-router
#       enable: true
#       path: /var/log/containers/client-*.log
#       refresh_interval: 10
#     - name: tunnel
#       enable: true
#       path: /var/log/containers/r1tunnel*.log
#       refresh_interval: 10
#   outputSearchHost: "elasticsearch-master"
#   fullnameOverride: "fluent-bit"
#   outputSearchType: "es"
#   flush: 1
#   logLevel: info
#   metricsPort: 2020
#   service:
#     type: ClusterIP
#   prometheusRule:
#     enabled: false
#   serviceMonitor:
#     enabled: false
#   existingConfigMap: fluent-bit-config

fluent-bit:
  enabled: true
  fullnameOverride: "fluent-bit"
  existingConfigMap: fluent-bit-config
  nodeSelector: {}
    # tenantname:
  logs:
    - name: eoc-backend
      enable: true
      path: /var/log/containers/eoc-backend-*.log
      refresh_interval: 10
    - name: eoc-orchestrator
      enable: true
      path: /var/log/containers/eoc-orchestrator-*.log
      refresh_interval: 10
    - name: sdc
      enable: true
      path: /var/log/containers/sdc-*.log
      refresh_interval: 10
    - name: client-router
      enable: true
      path: /var/log/containers/client-*.log
      refresh_interval: 10
    - name: tunnel
      enable: true
      path: /var/log/containers/r1tunnel*.log
      refresh_interval: 10
  outputs: []
    # - type: es  # Elasticsearch output plugin
    #   host: "elasticsearch-master"
    #   port: 9200
    #   index_prefix: "es-index"
    #   tls: false
    #   username: "elastic"
    #   password: "changeme"
    # - type: splunk  # Splunk output plugin
    #   host: "splunk-master"
    #   port: 8088
    #   splunk_token: "your-splunk-token"
    #   index_prefix: "splunk-index"
    #   tls: true
    #   tls_verify: false
    #   # Splunk output plugin uses Splunk_Token for authentication
    # - type: opensearch  # OpenSearch output plugin
    #   host: "opensearch-master"
    #   port: 9200
    #   index_prefix: "opensearch-index"
    #   tls: false
    #   username: "admin"
    #   password: "admin"
  outputSearchType: "es"
  outputSearchHost: "elasticsearch-master"
  outputSearchPort: 9200  # Default port for Elasticsearch
  # outputSearchTLS: false
  # outputSearchTLSVerify: true
  # outputSearchUsername: "elastic"  # Optional username
  # outputSearchPassword: "changeme"  # Optional password
  flush: 1
  logLevel: info
  metricsPort: 2020
  service:
    type: ClusterIP
  prometheusRule:
    enabled: false
  serviceMonitor:
    enabled: false



# Curator Configuration
# --------------------
# This section controls the behavior of Elasticsearch Curator, a tool for managing Elasticsearch indices.
# Curator can perform actions like deleting, closing, or creating indices based on configured filters.

curator:
  enabled: true
  dryrun: false
  # Elasticsearch Client Settings
  # -----------------------------
  # Specify the connection details for your Elasticsearch cluster.
  client:
    hosts: ["elasticsearch-master"]
    port: 9200
    use_ssl: false
    ssl_no_validate: true
    timeout: 300
    certificate: ""
    client_cert: ""
    client_key: ""
    # set username for es authentication
    username: ""
    # set password for es authentication
    password: ""
    master_only: false
  logging:
    # Curator Logging Configuration
    # ------------------------------
    # Customize how Curator logs its activities.
    # Set the desired log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
    loglevel: "INFO"
    logfile: ""
    logformat: "default"
    blacklist: ['elasticsearch', 'urllib3']
# Index Management Actions
# -------------------------
# Define a list of actions to be performed on Elasticsearch indices.
# Each action can have filters to select specific indices based on name patterns or age.
  logs:
    - name: "vds_server.log"
      # action: "delete_indices"
      # description: "Delete vds_server log files"
      # unit_count: 30
      # timeout_override: "120s"
    - name: "vds_server_access.log"
    - name: "adap_access.log"
    - name: "adap.log"
    - name: "web.log"
    - name: "web_access.log"
    - name: "event.log"
    - name: "periodiccahe.log"
    - name: "admin_rest_api_access.log"
    - name: "sync_engine.log"
    - name: "alerts.log"
    # - name: "close-old-indices"
    #   action: "close"
    #   description: "Close indices older than a year"
    #   delete_aliases: false
    #   skip_flush: true
    #   ignore_sync_failures: true
    #   timeout_override: "60s"
    #   value: "old-"
    #   unit: "months"
    #   unit_count: 12
    #   direction: "older"
    # - name: "create-new-index"
    #   action: "create_index"
    #   description: "Create a new index for upcoming logs"
    #   extra_settings:
    #     settings:
    #       number_of_shards: 3
    #       number_of_replicas: 1
    #   unit: "days"
    #   unit_count: 1
  cronjob:
    # Curator Cron Job Settings
    # --------------------------
    # Configure the Kubernetes CronJob that will trigger Curator actions.
    # Cron schedule (e.g., every minute)
    schedule: "* * * * *"
    annotations: {}
    labels: {}
    concurrencyPolicy: ""
    failedJobsHistoryLimit: ""
    successfulJobsHistoryLimit: ""
    jobRestartPolicy: Never
    startingDeadlineSeconds: ""
  pod:
    annotations: {}
  rbac:
    enabled: true
  serviceAccount:
    create: true
    annotations: {}
  hooks:
    install: false
    upgrade: false
  priorityClassName: ""
  securityContext:
    runAsUser: 16
  psp:
    create: false
  resources: {}
  nodeSelector: {}
    # tenantname:

# Velero Backup and Disaster Recovery Configuration
# ------------------------------------------------
#
# This section enables deployment of Velero, a Kubernetes backup and disaster recovery tool.
#
# Key Features:
# - Backup and restore Kubernetes resources and persistent volumes
# - Scheduled backups with configurable frequency and retention policies
# - Support for various backup storage providers (e.g., AWS S3, Azure Blob Storage, etc.)
# - Plugin system for extending functionality
#
# Key Configuration:
# - `backupStorage`: Define the storage location for backups (e.g., S3 bucket).
# - `credentials`: Configure credentials for the backup storage provider (if needed).
# - `defaultBackupTTL`: Set the default retention period for backups.
#
# Note: Ensure proper configuration of backup storage and credentials for successful operation.

velero:
  enabled: false
  fullnameOverride: "velero"
  backupStorage:
    bucket: ""
    region: ""
    # defines how frequently Velero should validate the object storage
    validationFrequency: 1h
    # prefix is the directory under which all Velero data should be stored within the bucket
    prefix:
  credentials:
    useSecret: false
  cleanUpCRDs: true
  nodeSelector: {}
  configuration:
    # defaultBackupStorageLocation: common-bsl
    # defaultVolumeSnapshotLocations: common-vsl
    # how long to wait by default before backups can be garbage collected
    defaultBackupTTL: 168h
    # the frequency of the reconciliation loop that garbage the expired backups
    garbageCollectionFrequency: 1h
    # avoid default Backup Storage Location to be created by Velero chart
    backupStorageLocation: nil
    # avoid default Volume Storage Location to be created by Velero chart
    volumeSnapshotLocation: nil
  initContainers:
    - name: velero-plugin-for-aws
      image: "velero/velero-plugin-for-aws:v1.8.2"
      imagePullPolicy: IfNotPresent
      volumeMounts:
        - mountPath: /target
          name: plugins

# Zoo Navigator Configuration
# ----------------------------
#
# This section enables deployment of Zoo Navigator, a web-based tool for visualizing and managing Apache ZooKeeper clusters.
#
# Key Features:
# - Single-pod deployment of Zoo Navigator
# - Configurable Docker image repository, tag, and pull policy
# - Service definition for exposing Zoo Navigator to the network
# - Optional settings for pod and container security
# - Resource limits and requests for CPU and memory
# - Optional horizontal autoscaling based on CPU/memory utilization
# - Node selector, tolerations, and affinity controls for pod scheduling
#
# Note: Review and adjust settings according to your ZooKeeper cluster setup and requirements.

zoonavigator:
  enabled: false
  replicaCount: 1
  image:
    repository: elkozmon/zoonavigator
    pullPolicy: Always
    # Overrides the image tag whose default is the chart appVersion.
    tag: "latest"
  imagePullSecrets: []
  podAnnotations: {}
  podSecurityContext: {}
    # fsGroup: 2000
  securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000
  service:
    type: ClusterIP
    port: 80
  resources: {}
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 3
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
  nodeSelector: {}
  tolerations: []
  affinity: {}

# -----------------------------------------------------------------
# -- Cloud Native PG
# -----------------------------------------------------------------
cloudnative-pg:
  enabled: false
  fullnameOverride: cnpg
  config:
    create: true
    name: cnpg-controller-manager-config
    data:
      INHERITED_LABELS: app.kubernetes.io/*, radiantlogic.io/*
      INHERITED_ANNOTATIONS: meta.helm.sh/*, helm.sh/*
  nodeSelector: {}
  tolerations: []
  affinity: {}
