nodeSelector: {}

ingress:
  enabled: false
  className: alb
  annotations: {}
  hosts:
    - host: chart-example.local
      paths:
        - "/"
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

argo-cd:
  enabled: true
  fullnameOverride: argocd
  configs:
    params:
      server.rootpath: "/argocd"
      server.insecure: true
  dex:
    enabled: false
  applicationSet:
    enabled: false
  notifications:
    enabled: false
  controller:
    nodeSelector: {}
  redis:
    nodeSelector: {}
  server:
    nodeSelector: {}
    service:
      type: NodePort
  repoServer:
    nodeSelector: {}
  crds:
    keep: false

prometheus:
  enabled: true
  server:
    extraFlags:
      - web.enable-lifecycle
      - web.route-prefix=/
      - web.external-url=http://prometheus-server/prometheus/
    fullnameOverride: prometheus-server
    configmapReload:
      enabled: false
    nodeSelector: {}
    # Persistence enabled by default and size to 50Gi
    persistentVolume:
      size: 8Gi
    statefulSet:
      enabled: true
  nodeExporter:
    enabled: false
  kubeStateMetrics:
    enabled: false
  alertmanager:
    enabled: false
    fullnameOverride: prometheus-alertmanager
    nodeSelector: {}
    replicationCount: 1
    configmapReload:
      ## If false, the configmap-reload container will not be deployed
      ##
      enabled: false
    config:
      global:
        smtp_from: 'saas@radiantlogic.com'
        smtp_smarthost: 'smtp-server:25'
        smtp_auth_username: ''
        smtp_auth_password: ''
        slack_api_url: https://hooks.slack.com/services/XXXXXXXXXXXX
      templates:
      - '/etc/alertmanager/*.tmpl'
      route:
        receiver: 'team-fid-devops'
        group_by: ['alertname']
        group_wait: 30s
        group_interval: 3h
        repeat_interval: 3h
        routes:
        - matchers:
          - monitor=~"fid|radiantone|vds"
          receiver: team-fid-devops
      inhibit_rules:
      - source_matchers: [severity="critical"]
        target_matchers: [severity="warning"]
        equal: [alertname]
      receivers:
      - name: 'team-fid-devops'
        email_configs:
        - to: 'username_to@example.com'
        slack_configs:
        - channel: '#devops'
          send_resolved: true
          title: |-
            [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }} for {{ .CommonLabels.job }}
            {{- if gt (len .CommonLabels) (len .GroupLabels) -}}
              {{" "}}(
              {{- with .CommonLabels.Remove .GroupLabels.Names }}
                {{- range $index, $label := .SortedPairs -}}
                  {{ if $index }}, {{ end }}
                  {{- $label.Name }}="{{ $label.Value -}}"
                {{- end }}
              {{- end -}}
              )
            {{- end }}
          text: >-
            {{ range .Alerts -}}
            *Alert:* {{ .Annotations.title }}{{ if .Labels.severity }} - `{{ .Labels.severity }}`{{ end }}
            *Description:* {{ .Annotations.description }}
            *Details:*
              {{ range .Labels.SortedPairs }} â€¢ *{{ .Name }}:* `{{ .Value }}`
              {{ end }}
            {{ end }}
        # alertmanager.tmpl: |-
  serverFiles:
    alerting_rules.yml:
      groups:
        - name: FID alerts
          rules:
          - alert: FID memory is high
            expr: (ldap_memory_used{job="fid-targets"}/ldap_memory_max{job="fid-targets"}) * 100 > 80
            for: 120s
            labels:
              severity: warning
            annotations:
              title: FID Memory usage on {{ $labels.instance }} is more than 80%
              description: warning! {{ $labels.job }} on {{ $labels.instance }} found FID memory usage is more than 80% for 120s
          - alert: FID connections are high
            expr: ldap_current_connections{job="fid-targets"} > 800
            for: 10s
            labels:
              severity: warning
            annotations:
              title: FID connections on {{ $labels.instance }} are more than 800
              description: warning! {{ $labels.job }} on {{ $labels.instance }} found FID connections are morethan 5 for 10s
          - alert: FID node disk usage is high
            expr: (ldap_disk_used{job="fid-targets"}/ldap_disk_total{job="fid-targets"}) * 100 > 80
            for: 120s
            labels:
              severity: warning
            annotations:
              title: FID node disk usage on {{ $labels.instance }} is more than 80%
              description: warning! {{ $labels.job }} on {{ $labels.instance }} found FID node disk usage are more than 80% for 120s

  pushgateway:
    fullnameOverride: prometheus-pushgateway
    nodeSelector: {}

grafana:
  fullnameOverride: grafana
  enabled: true
  nodeSelector: {}
  # Persistence enabled by default
  persistence:
    enabled: true
    size: 8Gi
  grafana.ini:
    server:
      root_url: "%(protocol)s://%(domain)s/grafana"
      serve_from_sub_path: true
    auth.anonymous:
      enabled: true
      org_role: Viewer
    analytics:
      check_for_updates: false
    panels:
      disable_sanitize_html: true
    log:
      mode: console
    log.console:
      format: text
      level: info
    security:
      allow_embedding: true

  # Setup Data Source (prometheus and elastic)
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          url: http://prometheus-server
          access: proxy
          isDefault: true
        - name: Elasticsearch
          type: elasticsearch
          database: vds_server_access.log*
          url: http://elasticsearch-master:9200
          password: ""
          user: ""
          access: proxy
          isDefault: false
          jsonData:
            esVersion: '7.17.3'
            logLevelField: fields.level
            logMessageField: message
            maxConcurrentShardRequests: 5
            timeField: '@timestamp'
          readonly: true
        - name: Alertmanager
          type: alertmanager
          url: http://prometheus-alertmanager
          access: proxy

  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'fid'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        updateIntervalSeconds: 10
        allowUiUpdates: true
        editable: true
        options:
          path: /var/lib/grafana/dashboards/fid
      - name: 'zookeeper'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        updateIntervalSeconds: 10
        allowUiUpdates: true
        editable: true
        options:
          path: /var/lib/grafana/dashboards/zookeeper
      - name: 'elasticsearch'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        updateIntervalSeconds: 10
        allowUiUpdates: true
        editable: true
        options:
          path: /var/lib/grafana/dashboards/elasticsearch
  dashboardsConfigMaps:
    fid: "fid-dashboard"
    zookeeper: "zookeeper-dashboard"
    elasticsearch: "audit-logs-elastic-dashboard"

elasticsearch:
  enabled: true
  replicas: 1
  nodeSelector: {}
  # Persistence enabled by default and size to 100Gi
  volumeClaimTemplate:
    resources:
      requests:
        storage: 30Gi

kibana:
  enabled: true
  fullnameOverride: kibana
  nodeSelector: {}
  kibanaConfig:
    kibana.yml: |
      server.basePath: "/kibana"
      telemetry.optIn: false

haproxy:
  enabled: true
  fullnameOverride: haproxy
  nodeSelector: {}
  service:
    type: NodePort
  config: |
    defaults
      timeout connect 10s
      timeout client 30s
      timeout server 30s
      log global
      mode http
      option httplog
      maxconn 3000
    frontend http-in
      bind *:80

      stats enable
      stats refresh 30s
      stats show-node
      stats uri /stats
      monitor-uri /healthz

      use_backend argocd_backend if { path /argocd } or { path_beg /argocd/ }
      use_backend grafana_backend if { path /grafana } or { path_beg /grafana/ }
      use_backend kibana_backend if { path /kibana } or { path_beg /kibana/ }
      #use_backend elasticsearch_backend if { path /elasticsearch } or { path_beg /elasticsearch/ }
      #use_backend prometheus_backend if { path /prometheus } or { path_beg /prometheus/ }
      #use_backend alertmanager_backend if { path /alertmanager } or { path_beg /alertmanager/ }
      #use_backend pushgateway_backend if { path /pushgateway } or { path_beg /pushgateway/ }
      #use_backend pgadmin4_backend if { path /pgadmin4 } or { path_beg /pgadmin4/ }
      #use_backend slamd_backend if { path /slamd } or { path_beg /slamd/ }
      #use_backend shellinabox_backend if { path /shellinabox } or { path_beg /shellinabox/ }
      #use_backend eocui_backend if { path /eoc } or { path_beg /eoc/ }
      #use_backend eocapi_backend if { path /eoc-backend } or { path_beg /eoc-backend/ }
      #use_backend sdcapi_backend if { path /sdc } or { path_beg /sdc/ }

    backend argocd_backend
      server argocd argocd-server:80
    backend grafana_backend
      http-request set-path %[path,regsub(^/grafana/?,/)]
      server grafana grafana:80
    backend kibana_backend
      http-request set-path %[path,regsub(^/kibana/?,/)]
      server kibana kibana:5601
    backend elasticsearch_backend
      http-request set-path %[path,regsub(^/elasticsearch/?,/)]
      server elasticsearch elasticsearch-master:9200
    backend prometheus_backend
      http-request set-path %[path,regsub(^/prometheus/?,/)]
      server prometheus prometheus-server:80
    #backend alertmanager_backend
      #http-request set-path %[path,regsub(^/alertmanager/?,/)]
      #server alertmanager prometheus-alertmanager:80
    backend pushgateway_backend
      http-request set-path %[path,regsub(^/pushgateway/?,/)]
      server pushgateway prometheus-pushgateway:9091
    #backend pgadmin4_backend
      #server pgadmin4 pgadmin4:80
    backend slamd_backend
      server slamd slamd:80
    backend shellinabox_backend
      http-request set-path %[path,regsub(^/shellinabox/?,/)]
      server shellinabox shellinabox:8080
    #backend eocui_backend
      #server eocui eoc-ui-service:80
    #backend eocapi_backend
      #server eocapi eoc-backend-service:80
    #backend sdcapi_backend
      #server sdcapi sdc-agent:80

postgresql:
  enabled: true
  fullnameOverride: postgresql
  primary:
    nodeSelector: {}
    # Persistence enabled by default and size to 50Gi
    persistence:
      size: 10Gi
    initdb:
      scriptsConfigMap: "postgres-init-script"
  databases:
    eoc:
      databaseName: eocdb
      user: eocadmin
      password: TSXojYsPF4AeZgTq
      schema: eoc
    sdc:
      databaseName: agentsdb
      user: agentsadmin
      password: iJukleKLG9fNihIQ
      schema: agents

pgadmin4:
  enabled: true
  fullnameOverride: pgadmin4
  nodeSelector: {}
  persistentVolume:
    enabled: false
  env:
    contextPath: "/pgadmin4"

zookeeper:
  enabled: false
  fullnameOverride: zookeeper
  nodeSelector: {}

slamd:
  enabled: true
  replicaCount: 1
  image:
    repository: pgodey/slamd
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: "latest"
  imagePullSecrets: []
  podAnnotations: {}
  podSecurityContext: {}
    # fsGroup: 2000
  securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000
  service:
    type: ClusterIP
    port: 80
  resources: {}
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80
  nodeSelector: {}
  tolerations: []
  affinity: {}

  client:
    replicaCount: 0
    image:
      repository: pgodey/slamd-client
      pullPolicy: IfNotPresent
      # Overrides the image tag whose default is the chart appVersion.
      tag: "latest"
    podAnnotations: {}
    podSecurityContext: {}
    # fsGroup: 2000
    securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000
    resources: {}
    autoscaling:
      enabled: false
      minReplicas: 1
      maxReplicas: 100
      targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80
    nodeSelector: {}
    tolerations: []
    affinity: {}

shellinabox:
  enabled: true
  replicaCount: 1
  image:
    repository: sspreitzer/shellinabox
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: "ubuntu"
  imagePullSecrets: []
  podAnnotations: {}
  podSecurityContext: {}
    # fsGroup: 2000
  securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000
  service:
    type: ClusterIP
    port: 8080
  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

smtp:
  enabled: true
  replicaCount: 1
  image:
    repository: bytemark/smtp
    tag: "latest"
    # Overrides the image tag whose default is the chart appVersion.
    pullPolicy: IfNotPresent
  imagePullSecrets: []
  podAnnotations: {}
  podSecurityContext: {}
    # fsGroup: 2000
  securityContext: {}
  service:
    type: ClusterIP
    port: 8080
  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}
  relay:
    enabled: true
    host: "smtp.sendgrid.net"
    port: "587"
    username: ""
    password: ""

opensearch:
  enabled: true
  fullnameOverride: "opensearch"
  replicas: 3
  clusterName: "opensearch-cluster"
  nodeGroup: "master"
  masterService: "opensearch-cluster-master"
  extraEnvs:
    - name: "DISABLE_SECURITY_PLUGIN"
      value: "true"
  rbac:
    create: false
    serviceAccountAnnotations: {}
    serviceAccountName: ""
  nodeSelector: {}
  # tenantname: duploservices-nike-svc
  podSecurityContext:
    fsGroup: 1000
    runAsUser: 1000
  service:
    type: ClusterIP
    nodePort: ""
    annotations: {}
    httpPortName: http
    transportPortName: transport
  extraVolumes: []
  # - name: extras
  #   emptyDir: {}
  extraVolumeMounts: []
    # - name: extras
    #   mountPath: /usr/share/extras
    #   readOnly: true
  extraContainers: []
    # - name: do-something
    #   image: busybox
    #   command: ['do', 'something']
  extraInitContainers: []
    # - name: do-somethings
    #   image: busybox
    #   command: ['do', 'something']

opensearch-dashboards:
  enabled: true
  opensearchHosts: "http://opensearch-cluster-master:9200"
  replicaCount: 1
  fullnameOverride: "opensearch-dashboards"
  extraEnvs:
    - name: DISABLE_SECURITY_DASHBOARDS_PLUGIN
      value: "true"
  service:
    type: ClusterIP
    port: 5601
    loadBalancerIP: ""
    nodePort: ""
    labels: {}
    annotations: {}
    loadBalancerSourceRanges: []
    # 0.0.0.0/0
    httpPortName: http
  nodeSelector: {}
    # tenantname: duploservices-nike-svc
  plugins:
    enabled: false
    installList: []
    # - example-fake-plugin-downloadable-url

fluent-bit:
  enabled: true
  fullnameOverride: "fluent-bit"
  service:
    type: ClusterIP
  prometheusRule:
    enabled: false
  serviceMonitor:
    enabled: false
  nodeSelector: {}
  config:
    service: |
      [SERVICE]
          Daemon Off
          Flush {{ .Values.flush }}
          Log_Level {{ .Values.logLevel }}
          Parsers_File parsers.conf
          Parsers_File custom_parsers.conf
          HTTP_Server On
          HTTP_Listen 0.0.0.0
          HTTP_Port {{ .Values.metricsPort }}
          Health_Check On
    inputs: |
      [INPUT]
          Name              tail
          Tag               kube.sdc*
          Path              /var/log/containers/sdc*.*
          Exclude_Path      /var/log/containers/fluent-bit*, /var/log/containers/aws-node*, /var/log/containers/kube-proxy*
          Parser            docker
          DB                /var/log/flb_kube.db
          Skip_Long_Lines   On
          Refresh_Interval  10
      [INPUT]
          Name              tail
          Tag               kube.uplink*
          Path              /var/log/containers/uplink-operator*.*
          Exclude_Path      /var/log/containers/fluent-bit*, /var/log/containers/aws-node*, /var/log/containers/kube-proxy*
          Parser            docker
          DB                /var/log/flb_kube2.db
          Skip_Long_Lines   On
          Refresh_Interval  10
    filters: |
      [FILTER]
          Name kubernetes
          Match kube.*
          Merge_Log On
          Keep_Log Off
          K8S-Logging.Parser On
          K8S-Logging.Exclude off
          Labels On
          Annotations On
    outputs: |
      [OUTPUT]
          Name               es
          Match              kube.sdc*
          Host               elasticsearch-master
          Trace_Error        On
          Replace_Dots       On
          Logstash_Format    On
          Logstash_Prefix    sdc
          Retry_Limit        False
      [OUTPUT]
          Name               es
          Match              kube.uplink*
          Host               elasticsearch-master
          Trace_Error        On
          Replace_Dots       On
          Logstash_Format    On
          Logstash_Prefix    uplink
          Retry_Limit        False
    customParsers: |
      [PARSER]
          Name docker_no_time
          Format json
          Time_Keep Off
          Time_Key time
          Time_Format %Y-%m-%dT%H:%M:%S.%L
  # The config volume is mounted by default, either to the existingConfigMap value, or the default of "fluent-bit.fullname"
  volumeMounts:
    - name: config
      mountPath: /fluent-bit/etc/fluent-bit.conf
      subPath: fluent-bit.conf
    - name: config
      mountPath: /fluent-bit/etc/custom_parsers.conf
      subPath: custom_parsers.conf
